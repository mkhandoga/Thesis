\section{Introduction}

\subsection{Historical retrospective}
    The reductionistic idea that all the countless variety of matter types that surrounds us could be in fact brought to a combination of much fewer substances has been around at least since the time of Ancient Greece. A thought that you can construct everything you see around out of one or few (e.g. fire, earth, water and air) indivisible elements ($\alpha \tau o \mu o \zeta$ in Greek) is simple, logical and therefore conceptually attractive. Knowing all about these elements could potentially grant us profound understanding of nature. But it wasn't before the XIX century when this idea has become something more than a philosophical concept and obtained solid scientific evidence. \\
The composition of the periodic table of elements in 1860s \cite{mendel} was a tremendous step forward, reducing the number of elements to O(100). The elements of the periodic table resembled the ancient Greek concept so much, that they were christened atoms. But the periodic character of the table and strong correlation of atom position in the table with its chemical properties was insinuating on a certain inner structure of the atoms, a possibility for them to be composed out of even smaller objects. The discovery of isotopes in 1913 \cite{isotopes} left little room for other explanation.\\
Further evidences in favour of atomistic views kept coming in late XIX and early XX centuries from theoretical and experimental sides. The molecular kinetic theory has been heavily criticized throughout the XIX century, but the explanation of the Brownian motion \cite{brownian} has secured its dominance from there on lying a foundation for what is to become the statistical physics. Of particular importance was the discovery of the first subatomic particle in 1897, which was called the electron \cite{cathode}. \\
Further studies of radioactive materials have allowed to compose a seemingly consistent understanding of what matter is composed of. By the time of neutron discovery in 1932 \cite{neutron} the list of what was called elementary particles was reasonably short: an electron, a proton, and a neutron. It was still left to figure out how these elements interact forming the known atoms, molecules and all the matter around. That required additional efforts on the theoretical side, including resolving the inconsistencies between the two new branches of physics supposed to describe the microworld and the fields, namely the quantum theory and the field theory. \\
To move forward the physicist have made use of another source of elementary particles - the cosmic rays. Cosmic rays contained particles of much higher energies comparing to the radioactive materials. Cosmic ray experiments have led to the discovery of the first known antiparticle - the positron \cite{positron_exp}, confirming the theoretical predictions by Dirac \cite{positron_th}. Further discoveries of the muon \cite{muon_exp}, pion \cite{pion}, kaon \cite{kaon} and $\Lambda_0$ \cite{lambda0} have shown that the list of elementary particles was still far from being completed. \\
The second half of the XX century has pronounced a new era in particle physics with the extensive use of particle accelerators. Accelerators have become the main experimental tool in the discovery of new particles and investigation of their properties. Comparing to the cosmic rays, accelerators could offer higher energies and better control over the experimental conditions. Thanks to these new tools by the end of 1960s the number of newly discovered particles has exceeded one hundred and kept growing, apparently taking away the reductionistic dream of having a reasonably small number of elementary particles. \\
On the other hand, the properties of the newly discovered particles (sometimes called "the particle zoo") had provided enough experimental data for theorists to make further assumptions. The particles, if grouped by their properties, have formed patterns - a situation resembling the old story with the atoms of the periodic table. This observation has allowed to assume the existence of even smaller fundamental particles with a fractional charge that would make up all the visible hadrons. These particles were eventually called quarks \cite{gellMann}, \cite{zweig}. By the late 1960s hypothesizing the existence of only three quarks was enough to explain all the visible particles and successfully predict new ones \cite{omega}. Since then three more quarks were discovered and as of now all the experimental evidence suggests that the quarks are truly fundamental particles being indivisible in the Ancient Greek sense. \\
At the same time serious theoretical efforts were taken in order to describe the interactions between fundamental particles, taking into account the known fundamental forces. In the mid-1970s a theory called The Standard Model was finalized. It included three out of four known fundamental forces (excluding the gravity) and predicted a number of particles which were not discovered by that time. All the key predictions of the theory were successfully confirmed by further experiments, making it a dominant theory in particle physics. The theory was able to describe all the surrounding matter with only 12 fundamental fermions (and their antiparticles) and 5 bosons. The \gls{sm} is described in more detail in the Chapter 1.\\
Theoretical efforts aimed to further simplify the list of fundamental particles are ongoing, but up to the time of this thesis writing none of them were confirmed experimentally. 
\subsection{Actual challenges}

The establishment of the Standard Model was a colossal step forward in understanding of the microworld physics. Nevertheless despite its great success and very good agreement with vast majority of the experimental data there is a number inconsistencies and lacunae in the theory, which do not allow to think of the \gls{sm} as of the final theory. Here are most notable of these problematic questions:
\begin{enumerate}
	\item A number of neutrino experiments have established that the neutrinos have a tiny though non-zero mass. The minimal Standard Model assumes neutrinos to be massless and does not allow to provide mass to the neutrinos. 
	\item Astrophysical and cosmological evidences confirm the existence of the dark matter which does not correspond to any of the \gls{sm} particles. 
	\item Cosmological observations show a substantial disproportion between observed matter and antimatter in favour of the former. The \gls{sm} does not provide an explanation how such an imbalance could have been formed.
	\item The discovery of the gravitational waves in 2016 had confirmed the existence of the graviton - the mediator of the gravitational force. The gravitational force is not represented in any way in the \gls{sm}.
	\item No explanation is provided to the vastly different magnitude of the fundamental forces, i.e. why the gravity is $10^{24}$ times weaker than the weak force. 
\end{enumerate}

In order to attack these and other problems numerous efforts have been taken to either modify the \gls{sm} or to replace it with a more fundamental theory, but so far none of these \gls{bsm} theories were ever confirmed experimentally. The \gls{sm} is still a source of most accurate predictions for any physical process that involves elementary particle interactions. Description of the \gls{bsm} theories goes beyond the scope of current thesis. \\
The \gls{sm} depends on the list of 18 free parameters (to be described in more detail in Chapter 1). These parameters can not be calculated intrinsically and must be measured experimentally. The more precisely we know the values of these parameters - the better is the accuracy of the \gls{sm} prediction. Precise knowledge of the \gls{sm} input parameters can also give hints on where to look for a more fundamental theory. \\
The LHC experiments have already contributed greatly by discovering the last missing piece of the \gls{sm}, the Higgs boson. This has ended the era of \gls{sm} particle discoveries but at the same time started the era of LHC precision measurements. The LHC experiments were capable to measure some parameters of the \gls{sm} for the first time (like the mass of the Higgs boson), but also could improve the existing measurements, boosting the predictive power of the \gls{sm}. \\
This thesis is a part of an ongoing effort at the ATLAS experiment to improve the precision of the W boson mass, which is also among the \gls{sm} free parameters. The mass of the W boson was first measured at \gls{lep} after its discovery in 1983. The precision of the measurement was further improved by the experiments at Tevatron collider. The only LHC result performed so far was published by ATLAS collaboration in 2018.\\
Hadron colliders are a challenging environment for the W boson-related measurements, the precision is highly impacted by a number of factors one of them being the pile-up. Current analysis is based on the data collected during two special LHC runs with low pile-up, taken in 2017 and 2018.  

\subsection{Thesis composition}
The first chapter contains the description of the Standard Model, its constituents and input parameters. Chapter 2 is dedicated to W boson and its properties. Chapter 3 tells about the \gls{lhc} and its operations. ATLAS detector is described in Chapter 4. Chapter 5 is dedicated to the description of the shower shapes reweighting. And so on and so forth...
\section{Introduction}

\subsection{Historical retrospective}
    The reductionistic idea that the countless varieties of matter types that surround us could be in fact simplified to a combination of much fewer substances has existed at least since the time of Ancient Greece. The thought that you can construct everything you see around out of one or few (e.g. fire, earth, water and air) indivisible elements ($\alpha \tau o \mu o \sigma$ in Greek) is simple, logical and therefore conceptually attractive. Knowing all about these elements could potentially grant us profound understanding of nature. But it wasn't before the XIX$^{th}$ century that this idea has become something more than a philosophical concept and obtained solid scientific evidence. \\

The composition of the periodic table of elements that has begun in the early XIX$^{th}$ century and concluded in the 1860s~\cite{mendel} was a tremendous step forward, reducing the number of elements to O(100). A brilliant (yet not completely true) hypothesis that all the chemical elements are composed out of hydrogen atoms was published by William Prout as early as 1815~\cite{prout}. The elements of the periodic table resembled the ancient Greek concept so much, that they were christened atoms. But the periodic character of the table and strong correlation of atom position in the table with its chemical properties was insinuating on a certain inner structure of the atoms, a possibility for them to be composed out of even smaller objects. The discovery of isotopes in 1913~\cite{isotopes} left little room for other explanation.\\
Further evidence in favour of the atomistic view kept coming in the late XIX$^{th}$ and early XX$^{th}$ centuries from theoretical and experimental sides. The molecular kinetic theory has been heavily criticized throughout the XIX$^{th}$ century, but the explanation of the Brownian motion~\cite{brownian} has secured its dominance from there on lying a foundation for what is to become the statistical physics. Of particular importance was the discovery of the first subatomic particle in 1897, which was called the electron \cite{cathode}. It was shortly followed by the identification of a hydrogen ion, subsequently named a proton~\cite{proton_discovery}\footnote{The anode rays were discovered in 1886, though they included diverse ions with different charge-to-mass ratios and could not be identified as a single particle~\cite{anode_rays}.}. Originating from the Greek word $\pi \rho \omega \tau o \sigma$ (primal), the name was inspired by the term \textit{protyle} introduced by William Prout a century before for the hydrogen atom~\cite{prout}.\\
Further studies of radioactive materials have allowed to compose a seemingly consistent understanding of what matter is composed of. By the time of neutron discovery in 1932 \cite{neutron} the list of what was called elementary particles was reasonably short: an electron, a proton, and a neutron. The list could arguably be supplemented with a hypothetical very light chargeless particle called neutrino, postulated in 1930 in order to explain the continuous electron spectrum in $\beta-$decay~\cite{pauli_neutrino}. It was still left to figure out how these elements interact forming the known atoms, molecules and all the matter around. That required additional efforts on the theoretical side, including resolving the inconsistencies between the two new branches of physics supposed to describe the microworld and the fields, namely the quantum theory and the field theory. \\
To move forward physicists have made use of another source of elementary particles - the cosmic rays. Cosmic rays contained particles of much higher energies comparing to the radioactive materials. Cosmic ray experiments have led to the discovery of the first known antiparticle - the positron \cite{positron_exp}, confirming the theoretical predictions by Dirac \cite{positron_th}. Further discoveries of the muon \cite{muon_exp}, pion \cite{pion}, kaon \cite{kaon} and $\Lambda_0$ \cite{lambda0} have shown that the list of elementary particles was still far from being completed. The experimental detection of neutrino in 1956~\cite{neutrino} has confirmed the theoretical prediction made over quarter of a century before.  \\
The second half of the XX$^{th}$ century has pronounced a new era in particle physics with the extensive use of particle accelerators. Accelerators have become the main experimental tool in the discovery of new particles and investigation of their properties. Comparing to the cosmic rays, accelerators could offer a more stable flow of high energy particles and better control over the experimental conditions. Thanks to these new tools by the end of 1960s the number of newly discovered particles has exceeded one hundred and kept growing, apparently taking away the reductionistic dream of having a reasonably small number of elementary particles. \\
On the other hand, the properties of the newly discovered particles (sometimes called "the particle zoo") had provided enough experimental data for theorists to make further assumptions. The particles, if grouped by their properties, have formed patterns - a situation resembling the old story with the atoms of the periodic table. This observation has allowed to assume the existence of even smaller fundamental particles with a fractional charge that would make up all the visible hadrons. These particles were eventually called quarks \cite{gellMann}, \cite{zweig}. By the late 1960s hypothesizing the existence of only three quarks was enough to explain all the visible particles and successfully predict new ones \cite{omega}. Since then three more quarks were discovered and as of now all the experimental evidence suggests that the quarks are truly fundamental particles being indivisible in the Ancient Greek sense. \\
At the same time serious theoretical efforts were taken in order to describe the interactions between fundamental particles, taking into account the known fundamental forces. In the mid-1970s a theory called The Standard Model was finalized. It included three out of four known fundamental forces (excluding the gravity) and predicted a number of particles which were not discovered by that time. All the key predictions of the theory were successfully confirmed by further experiments, making it a dominant theory in particle physics. The theory was able to describe all the surrounding matter with only 12 fundamental fermions (and their antiparticles) and 5 bosons. The \gls{sm} is described in more detail in Chapter 1.\\
Theoretical efforts aimed to further simplify the list of fundamental particles are ongoing, but up to the time of this thesis writing none of them were confirmed experimentally. 
\subsection{Current challenges}

The establishment of the Standard Model was a colossal step forward in the understanding of microworld physics. Nevertheless despite its great success and very good agreement with the vast majority of the experimental data there is a number inconsistencies and lacunae in the theory which do not allow to think of the \gls{sm} as of the final theory. Here are most notable of these problematic questions:
\begin{enumerate}
	\item A number of neutrino experiments have established that the neutrinos have a tiny though non-zero mass. The minimal Standard Model assumes neutrinos to be massless and does not allow to provide mass to the neutrinos. 
	\item Astrophysical and cosmological evidences confirm the existence of dark matter which does not correspond to any of the particles of the \gls{sm}. 
	\item Cosmological observations show a substantial disproportion between observed matter and antimatter in favour of the former. The \gls{sm} does not provide an explanation how such an imbalance could have been formed. This fact is probably connected to the problem of CP-violation, which also lacks explanation from the \gls{sm}.
	\item The discovery of the gravitational waves in 2016 have confirmed the existence of the graviton - the mediator of the gravitational force. The gravitational force is not represented in any way in the \gls{sm}.
	\item No explanation is provided to the vastly different magnitude of the fundamental forces, i.e. why the gravity is $10^{24}$ times weaker than the weak force. 
	\item Three generations of fermions are postulated with no explanation for number of generations. 
\end{enumerate}

In order to attack these and other problems numerous efforts have been taken to either modify the \gls{sm} or to replace it with a more fundamental theory, but so far none of these \gls{bsm} theories were ever confirmed experimentally. The \gls{sm} is still a source of most accurate predictions for any physical process that involves elementary particle interactions. Description of the \gls{bsm} theories goes beyond the scope of current thesis. \\
The \gls{sm} depends on the list of 18 free parameters if the neutrinos are assumed massless (25 parameters if neutrinos are massive). These parameters can not be calculated intrinsically and must be measured experimentally. The more precisely we know the values of these parameters - the better is the accuracy of the \gls{sm} prediction. Precise knowledge of the \gls{sm} input parameters can also give hints on where to look for a more fundamental theory. \\
The LHC experiments have already contributed greatly by discovering the last missing piece of the \gls{sm}, the Higgs boson. This has ended the era of \gls{sm} particle discoveries but at the same time started the era of LHC precision measurements. The LHC experiments were capable to measure some parameters of the \gls{sm} for the first time (like the mass of the Higgs boson), but also could improve the existing measurements, boosting the predictive power of the \gls{sm}. \\
The scope of this thesis includes the measurement of the W boson transverse momentum spectrum. This measurement may serve as a test for the \gls{sm} predictions for differential cross-sections. It is also an important part of an ongoing effort at the ATLAS experiment to improve the precision of the W boson mass measurement, which is also among the \gls{sm} free parameters. The mass of the W boson was first measured at \gls{lep} after its discovery in 1983. The precision of the measurement was further improved by the experiments at Tevatron collider. The only LHC result performed so far was published by ATLAS collaboration in 2018~\cite{wboson}.\\
Hadron colliders are a challenging environment for the W boson-related measurements, the precision is highly impacted by a number of factors one of them being pile-up. The current analysis is based on the data collected during two special LHC runs with low pile-up, taken in 2017 and 2018.  

\subsection{Thesis composition outline}
The first chapter contains the description of the Standard Model, its constituents and input parameters. Chapter 2 is dedicated to the W boson and its properties. The chapter contains description of vector bosons production at hadron colliders and theoretical approach to the cross-section derivation. ATLAS detector is described in Chapter 3. Chapter 3 tells about the \gls{lhc} and its operations. Chapter 5 is dedicated to the description of the shower shapes reweighting. And so on and so forth...
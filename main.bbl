% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{1}
  \datalist[entry]{none/global//global/global}
    \entry{gradient}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=d0ab8cbca75df7aa3e0b6024d60ac5f8}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=ac762f2592005edf6bad58f566967c6c}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={Léon},
           giveni={L\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=844e183692f08a63f99285330fcf90cc}{%
           family={Haffner},
           familyi={H\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{f7ce6caf843f284e299ac4d8b37d6357}
      \strng{fullhash}{2ec348724dbea153e32003064457e9e8}
      \strng{bibnamehash}{f7ce6caf843f284e299ac4d8b37d6357}
      \strng{authorbibnamehash}{f7ce6caf843f284e299ac4d8b37d6357}
      \strng{authornamehash}{f7ce6caf843f284e299ac4d8b37d6357}
      \strng{authorfullhash}{2ec348724dbea153e32003064457e9e8}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the IEEE}
      \field{title}{Gradient-based learning applied to document recognition}
      \field{year}{1998}
      \field{pages}{2278\bibrangedash 2324}
      \range{pages}{47}
    \endentry
    \entry{sgd1}{inbook}{}
      \name{author}{1}{}{%
        {{hash=ac762f2592005edf6bad58f566967c6c}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={Léon},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{ac762f2592005edf6bad58f566967c6c}
      \strng{fullhash}{ac762f2592005edf6bad58f566967c6c}
      \strng{bibnamehash}{ac762f2592005edf6bad58f566967c6c}
      \strng{authorbibnamehash}{ac762f2592005edf6bad58f566967c6c}
      \strng{authornamehash}{ac762f2592005edf6bad58f566967c6c}
      \strng{authorfullhash}{ac762f2592005edf6bad58f566967c6c}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Neural Networks: Tricks of the Trade}
      \field{month}{01}
      \field{title}{Stochastic Gradient Descent Tricks}
      \field{volume}{7700}
      \field{year}{2012}
      \field{pages}{421\bibrangedash 436}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/978-3-642-35289-8_25
      \endverb
    \endentry
    \entry{sgd2}{article}{}
      \name{author}{3}{}{%
        {{hash=c84f9d607e83d071830e9667dc3cb31f}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=881b7655f3886f90a9400902a521acdb}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=68500db67576d4d0a65080bb1c0f9b9b}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{fullhash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{bibnamehash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{authorbibnamehash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{authornamehash}{ba1d15d4121bba06b3a9707e415adefc}
      \strng{authorfullhash}{ba1d15d4121bba06b3a9707e415adefc}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Nature}
      \field{title}{Learning representations by back-propagating errors}
      \field{volume}{323}
      \field{year}{1986}
      \field{pages}{533\bibrangedash 536}
      \range{pages}{4}
    \endentry
    \entry{kingma2014method}{misc}{}
      \name{author}{2}{}{%
        {{hash=1e072d74f4ac1094c9c26e1b592cea4a}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{c1670936fba733d1cb2d4c23bce95c6e}
      \strng{fullhash}{c1670936fba733d1cb2d4c23bce95c6e}
      \strng{bibnamehash}{c1670936fba733d1cb2d4c23bce95c6e}
      \strng{authorbibnamehash}{c1670936fba733d1cb2d4c23bce95c6e}
      \strng{authornamehash}{c1670936fba733d1cb2d4c23bce95c6e}
      \strng{authorfullhash}{c1670936fba733d1cb2d4c23bce95c6e}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
      \field{note}{cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015}
      \field{title}{Adam: A Method for Stochastic Optimization}
      \field{year}{2014}
      \verb{urlraw}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
      \keyw{final thema:neural_attentional_rating_regression}
    \endentry
    \entry{grad_momentum1}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=acfacef8b6f3eefa3db0c10546612bf1}{%
           family={Nesterov},
           familyi={N\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{acfacef8b6f3eefa3db0c10546612bf1}
      \strng{fullhash}{acfacef8b6f3eefa3db0c10546612bf1}
      \strng{bibnamehash}{acfacef8b6f3eefa3db0c10546612bf1}
      \strng{authorbibnamehash}{acfacef8b6f3eefa3db0c10546612bf1}
      \strng{authornamehash}{acfacef8b6f3eefa3db0c10546612bf1}
      \strng{authorfullhash}{acfacef8b6f3eefa3db0c10546612bf1}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Soviet Mathematics. Doklady}
      \field{month}{01}
      \field{title}{A method of solving a convex programming problem with convergence rate $O(1/k^2)$}
      \field{volume}{27}
      \field{year}{1983}
      \field{pages}{372\bibrangedash 376}
      \range{pages}{5}
    \endentry
    \entry{grad_momentum2}{article}{}
      \name{author}{1}{}{%
        {{hash=356a60acbc57efd234492866fcb6695c}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{356a60acbc57efd234492866fcb6695c}
      \strng{fullhash}{356a60acbc57efd234492866fcb6695c}
      \strng{bibnamehash}{356a60acbc57efd234492866fcb6695c}
      \strng{authorbibnamehash}{356a60acbc57efd234492866fcb6695c}
      \strng{authornamehash}{356a60acbc57efd234492866fcb6695c}
      \strng{authorfullhash}{356a60acbc57efd234492866fcb6695c}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Ussr Computational Mathematics and Mathematical Physics}
      \field{month}{12}
      \field{title}{Some methods of speeding up the convergence of iteration methods}
      \field{volume}{4}
      \field{year}{1964}
      \field{pages}{1\bibrangedash 17}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1016/0041-5553(64)90137-5
      \endverb
    \endentry
    \entry{LeCun1998}{inbook}{}
      \name{author}{4}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=179581647e37c19b9a3deb51a1cfc5b0}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={Leon},
           giveni={L\bibinitperiod}}}%
        {{hash=df8a2d492926c0ba157376128774d796}{%
           family={Orr},
           familyi={O\bibinitperiod},
           given={Genevieve\bibnamedelima B.},
           giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=53f0b612af1a32a5946fd0f2ef47160b}{%
           family={M{ü}ller},
           familyi={M\bibinitperiod},
           given={Klaus\bibnamedelima -Robert},
           giveni={K\bibinitperiod\bibinitdelim \bibinithyphendelim \bibinithyphendelim R\bibinitperiod}}}%
      }
      \name{editor}{2}{}{%
        {{hash=df8a2d492926c0ba157376128774d796}{%
           family={Orr},
           familyi={O\bibinitperiod},
           given={Genevieve\bibnamedelima B.},
           giveni={G\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=9780dafc51f9232cce897369eb1b6692}{%
           family={M{ü}ller},
           familyi={M\bibinitperiod},
           given={Klaus-Robert},
           giveni={K\bibinithyphendelim R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{fullhash}{385f924fdad71cd3b4d40a369c9439d6}
      \strng{bibnamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authorbibnamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authornamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authorfullhash}{385f924fdad71cd3b4d40a369c9439d6}
      \strng{editorbibnamehash}{39ab8e2f834df2e1911c5d37a29fc323}
      \strng{editornamehash}{39ab8e2f834df2e1911c5d37a29fc323}
      \strng{editorfullhash}{39ab8e2f834df2e1911c5d37a29fc323}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.}
      \field{booktitle}{Neural Networks: Tricks of the Trade}
      \field{isbn}{978-3-540-49430-0}
      \field{title}{Efficient BackProp}
      \field{year}{1998}
      \field{pages}{9\bibrangedash 50}
      \range{pages}{42}
      \verb{doi}
      \verb 10.1007/3-540-49430-8_2
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/3-540-49430-8_2
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/3-540-49430-8_2
      \endverb
    \endentry
    \entry{Kurt1991251}{article}{}
      \name{author}{2}{}{%
        {{hash=ff8f3b2601338785ba30ac26188290df}{%
           family={Kurt},
           familyi={K\bibinitperiod}}}%
        {{hash=88449b9a7a7b28eb8e2a8676a0dc55ce}{%
           family={Hornik},
           familyi={H\bibinitperiod}}}%
      }
      \strng{namehash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \strng{fullhash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \strng{bibnamehash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \strng{authorbibnamehash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \strng{authornamehash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \strng{authorfullhash}{4aae92f49bfbeffeb08713fbc0e480d4}
      \field{sortinit}{9}
      \field{sortinithash}{1dd72ab054147731c9d824b49aba0534}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{{We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(渭) performance criteria, for arbitrary finite input environment measures 渭, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.}}
      \field{journaltitle}{Neural Networks}
      \field{number}{2}
      \field{title}{{Approximation capabilities of multilayer feedforward networks}}
      \field{volume}{4}
      \field{year}{1991}
      \field{pages}{251\bibrangedash 257}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1016/0893-6080(91)90009-T
      \endverb
      \verb{urlraw}
      \verb http://www.sciencedirect.com/science/article/pii/089360809190009T
      \endverb
      \verb{url}
      \verb http://www.sciencedirect.com/science/article/pii/089360809190009T
      \endverb
      \keyw{*file-import-12-02-28 approximation,duckling,feed-forward,feedforward,free,lunch,multilayer,network,networks,neural,no,theorem,ugly,universal}
    \endentry
    \entry{Cybenko1989}{article}{}
      \name{author}{1}{}{%
        {{hash=c842e44e58b470f560c8bd77995b47cc}{%
           family={Cybenko},
           familyi={C\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{c842e44e58b470f560c8bd77995b47cc}
      \strng{fullhash}{c842e44e58b470f560c8bd77995b47cc}
      \strng{bibnamehash}{c842e44e58b470f560c8bd77995b47cc}
      \strng{authorbibnamehash}{c842e44e58b470f560c8bd77995b47cc}
      \strng{authornamehash}{c842e44e58b470f560c8bd77995b47cc}
      \strng{authorfullhash}{c842e44e58b470f560c8bd77995b47cc}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}
      \field{issn}{1435-568X}
      \field{journaltitle}{Mathematics of Control, Signals and Systems}
      \field{month}{12}
      \field{number}{4}
      \field{title}{Approximation by superpositions of a sigmoidal function}
      \field{volume}{2}
      \field{year}{1989}
      \field{pages}{303\bibrangedash 314}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/BF02551274
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF02551274
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF02551274
      \endverb
    \endentry
    \entry{RumelhartZipser:86}{incollection}{}
      \name{author}{2}{}{%
        {{hash=cc138c7c1ecc0be7452e0db602712242}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={D.\bibnamedelimi E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=c1653d138fda2345602bc1c081b92fd6}{%
           family={Zipser},
           familyi={Z\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{038bc50cdd3c89cceb00bb56512f492c}
      \strng{fullhash}{038bc50cdd3c89cceb00bb56512f492c}
      \strng{bibnamehash}{038bc50cdd3c89cceb00bb56512f492c}
      \strng{authorbibnamehash}{038bc50cdd3c89cceb00bb56512f492c}
      \strng{authornamehash}{038bc50cdd3c89cceb00bb56512f492c}
      \strng{authorfullhash}{038bc50cdd3c89cceb00bb56512f492c}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Parallel Distributed Processing}
      \field{title}{Feature Discovery by Competitive Learning}
      \field{year}{1986}
      \field{pages}{151\bibrangedash 193}
      \range{pages}{43}
    \endentry
    \entry{dnn1}{article}{}
      \name{author}{7}{}{%
        {{hash=b934ab8a06f4ab846491dbac786745f9}{%
           family={Mehta},
           familyi={M\bibinitperiod},
           given={Pankaj},
           giveni={P\bibinitperiod}}}%
        {{hash=92b0f4441d84e9b7699803e1bf75a4a1}{%
           family={Bukov},
           familyi={B\bibinitperiod},
           given={Marin},
           giveni={M\bibinitperiod}}}%
        {{hash=44ea1f3f55f9c31b0baf1108b18e429e}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ching-Hao},
           giveni={C\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=5625d941a48ad9dffa76cc1bcd618014}{%
           family={Day},
           familyi={D\bibinitperiod},
           given={Alexandre\bibnamedelima G.R.},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=dfc166e5f89b55ae88e74b91a906c2c9}{%
           family={Richardson},
           familyi={R\bibinitperiod},
           given={Clint},
           giveni={C\bibinitperiod}}}%
        {{hash=de804f179ffa3f6b01ee3658994c5ee9}{%
           family={Fisher},
           familyi={F\bibinitperiod},
           given={Charles\bibnamedelima K.},
           giveni={C\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=dcadab036fffe58698f930dc7a2bc52b}{%
           family={Schwab},
           familyi={S\bibinitperiod},
           given={David\bibnamedelima J.},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{16810074b7b78357cd8a156cb1c46ec8}
      \strng{fullhash}{e112a5bdd86ff4541e326c053dcd6e62}
      \strng{bibnamehash}{16810074b7b78357cd8a156cb1c46ec8}
      \strng{authorbibnamehash}{16810074b7b78357cd8a156cb1c46ec8}
      \strng{authornamehash}{16810074b7b78357cd8a156cb1c46ec8}
      \strng{authorfullhash}{e112a5bdd86ff4541e326c053dcd6e62}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{physics.comp-ph}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Phys. Rept.}
      \field{title}{{A high-bias, low-variance introduction to Machine Learning for physicists}}
      \field{volume}{810}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 124}
      \range{pages}{124}
      \verb{doi}
      \verb 10.1016/j.physrep.2019.03.001
      \endverb
      \verb{eprint}
      \verb 1803.08823
      \endverb
    \endentry
    \entry{batch_normalization}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=5543e82359e26b035efc009cb3efff9d}{%
           family={Ioffe},
           familyi={I\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Lille, France}%
      }
      \list{publisher}{1}{%
        {JMLR.org}%
      }
      \strng{namehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{fullhash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{bibnamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authorbibnamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authornamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authorfullhash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37}
      \field{series}{ICML15}
      \field{title}{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}
      \field{year}{2015}
      \field{pages}{448\bibrangedash 456}
      \range{pages}{9}
    \endentry
  \enddatalist
\endrefsection
\endinput


\chapter{Hadronic recoil regression with deep neural networks}
In the recent years a significant progress was achieved in the field of big datasets analysis. The multivariant analysis techniques, given enough input data, are able to infer very complicated dependencies between the input and target variables. This has motivated me to use deep neural network (DNN) for the regression of the 2-component hadronic recoil vector.

The chapter opens with a theoretical introduction into the underlying principles of the feedforward neural networks used in the analysis. It is followed by the description of the network setup: inputs, hyperparameters and the training process. The results of the regression are benchmarked and discussed in the end of this chapter. The resulting plots for all the W channels are collected in Appendix B. 
\section{Deep neural networks}
Normally a machine learning problem has a number of ingredients: a dataset \textbf{X}, a set of parameters \textbf{$\theta$}, a model  $g(\theta)$ and a loss function $C(\textbf{X})$ that tells us how well the model $g(\textbf{\theta})$ describes the dataset. Finding the values of \textbf{$\theta$} that would minimize the loss function we fit the model.
\subsection{Gradient descent optimization}
One of the most powerful and used class of methods in minimizing the loss function is called the \textit{gradient descent}, \cite{gradient} especially its sub-class, the \gls{sgd} \cite{sgd1}, \cite{sgd2}. One of its modifications called ADAM \cite{kingma2014method} was used as an optimization algorithm in the work presented in this thesis. \\
Let's assume that a loss function $E(\theta)$ may be estimated as a sum over n data points:
\begin{equation}
E(\theta) = \sum^n_{i=1} e_i(x_i,\theta),
\end{equation}
where $x_i$ is a data point and $e_i$ is an estimate of performance. In the simplest case of the \gls{gd} algorithm we start looking for the values of parameters $\theta$ such that the sum of functions $\sum^n_{i=1}e_i$ is minimal. We start with a certain value $\theta_0$ and then iteratively perform the following:
\begin{equation}
\begin{array}{lcl} 
v_t=\eta_t\nabla_{\theta}E(\theta_t),\\
\theta_{t+1}=\theta_t-v_t,
\end{array} 
\end{equation}
where $\nabla_{\theta}E(\theta_t)$ is the gradient of $E(\theta)$ with respect to $\theta$; factor $\eta_t$ is called the \textit{learning rate} and defines the length of the step in the direction of $\theta$ performed with every iteration. Balancing learning rate is very important for learning process and convergence. A value too low can make the convergence "stuck" in a local minimum, it also increases the number of iterations. Picking a very high learning rate we risk to miss the minimum so the algorithm would never converge to a minimum. Also, if the number of data points $n$ is high, calculating the gradient is a costly task in terms of CPU time. 

Some of the problems accompanying the use of \gls{gd} are dealt with by using its modification - the \gls{sgd}. The idea is the following: instead of using all the available data points $n$ at each iteration of the \gls{gd}, we split the data into $k$ \textit{minibatches}, each having $M$ data points, such that $k = n/M$. Normally the size of the batch is ~few hundreds of data points, to provide a certain degree of variance and incorporating stochasticity. The transition to \gls{sgd} algorithm is done in the following way:
\begin{equation}
\nabla_{\theta}E(\theta) = \sum^n_{i=1} \nabla_{\theta}e_i(x_i,\theta) \rightarrow \sum_{i \in B_l} \nabla_{\theta}e_i(x_i,\theta),
\end{equation}
where $B_l$ is a set of data points belonging to a minibatch $l \in 1, ... , n/M$. Now every next iteration of $\theta$ parameters update is performed over a different batch, consecutively running over all the batches: 
\begin{equation}
\begin{array}{lcl} 
\nabla_{\theta}E^{EM}(\theta) = \sum_{i \in B_l} \nabla_{\theta}e_i(x_i,\theta),\\
v_t=\eta_t\nabla_{\theta}E^{EM}(\theta_t),\\
\theta_{t+1}=\theta_t-v_t.
\end{array} 
\end{equation}
A full iteration over all the $n/M$ batches is called an \textit{epoch}. Now stochasticity prevents the gradient algorithm from getting stuck in a local minimum. Also computing the gradient over fewer data point notably decreases the CPU time spent. 

The algorithm may be further improved, adding a "memory", that is to say making every next step $t$ dependent on the direction of the previous step $t-1$:
\begin{equation}
\begin{array}{lcl} 
v_t=\gamma v_{t-1}\eta_t\nabla_{\theta}E^{EM}(\theta_t),\\
\theta_{t+1}=\theta_t-v_t.
\end{array} 
\end{equation}
Because of an analogy from physics the parameter $\gamma$ is called a \textit{momentum}, having $0\le\gamma \le 1$ \cite{grad_momentum1}, \cite{grad_momentum2}. This parameter provides a certain "inertia" in the change of the direction of the gradient descent. Introduction of the momentum helps for quicker convergence in the case of a slow but steady change of a certain parameter during the gradient descent.

The convergence of the \gls{gd} may be significantly improved if the learning rate could be different in different directions, depending on the landscape of the parameter space $\theta$: the steeper the gradient in a certain direction - the smaller the corresponding step. The optimal step could be estimated by obtaining the \textit{Hessian matrix} in the vicinity of a point $\theta_0$, providing a description of the local curvature in a multidimensional space (although calculating Hessian matrix is complicated and slow-converging process \cite{LeCun1998}). However, a number of methods use the second moment of the gradient to efficiently estimate the optimal learning rate. One of such methods is called ADAM (ADAptive Momentum) \cite{kingma2014method}, its iterative relations are the following:
\begin{equation}
\begin{array}{lcl} 
g_t=\nabla_{\theta}E(\theta_t)\\
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t\\
s_t=\beta_2 s_{t-1} + (1-\beta_2)g_t^2\\
\hat{m_t}=\frac{m_t}{1-(\beta_1)^t}\\
\hat{s_t}=\frac{s_t}{1-(\beta_2)^t}\\
\theta_{t+1}=\theta_t-\eta_t\frac{\hat{m_t}}{\sqrt{\hat{s_t}}+\epsilon}.
\end{array} 
\end{equation}
Here the parameters $\beta_1$ and $\beta_2$ set the memory lifetime for the first and second moment; \eta is the learning rate and $\epsilon$ is a small regularization constant keeping the denominators from vanishing. Like in other cases of the \gls{sgd} here the iterations are performed batch-wise. Parameter $s_t$ is linked to the variance of the gradient size. This basically means that the learning rate is proportional to the first momentum of the gradient and inverse proportional to its standard deviation.
\subsection{DNN structure and training}
A neural network is composed of single neurons, also called nodes, arranged in layers. The first layer is called the input layer, the last one is called the output layer; all the layers in between are named hidden layers (see Fig. \ref{fig::dnn}). \\
A single node $i$ takes a vector of k input features $\textbf{x}=(x_1,x_2,...,x_k)$ and produces a scalar input $a_i(\textbf{x})$. Function $a_i$ may have a different form, although it normally can be decomposed into two steps. The first step is a linear transformation of the inputs into a scalar value assigning each input a weight:
\begin{equation}
z^{i}=w^{i}_k\cdot x_k+b^{i},
\end{equation}
where $\textbf{w}^i=(w^i_1,w^i_2,...,w^i_k)$ is a set of $k$ weights assigned to corresponding inputs. The weights $\textbf{w}^i$ are specific to a neuron $i$, as well as the scalar bias $b^i$. The next step is where the non-linear function $\sigma_i$ comes into play: we can express the output function $a_i(\textbf{x})$ as follows:
\begin{equation}
a_i(\textbf{x})=\sigma_i(z^{i}).
\end{equation}
There exists a number of options for the non-linear function $\sigma$; in current thesis a $\tanh$ is used. When the neurons are arranged in layers in a feed-forward neural network - the outputs from neurons of the previous layer serve as inputs for the succeeding layers neurons. The universal approximation theorem states, that a neural network with a single hidden layer can approximate any continuous multi-parametric function with arbitrary accuracy \cite{Kurt1991251}, \cite{Cybenko1989}. However, in practice it is easier to reach the possible precision having more hidden layers. 

So in terms of a \gls{dnn} fitting the model means tuning the weights and biases $(\textbf{w}^i,b^i)$ in such a way that a loss function applied to the new dataset would be minimal. It is reached through iterative process called \textit{training}, that involves the \gls{gd} with an algorithm called \textit{backpropagation} \cite{RumelhartZipser:86}. The backpropagation algorithm allows to calculate the gradients and adjust the corresponding parameters in a very computation-efficient way. 

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.5\textwidth,keepaspectratio]{dnn.png}
	\caption{A: The nodes perform a linear transformation of the inputs, then apply a non-linear activation function. B: The architecture of a deep neural network: neurons are arranged into layers~\cite{dnn1}. }
	\label{fig::dnn}
\end{figure}
Let us assume that there are L layers in the network $l=1,...,L$, that $w^l_{jk}$ and $b^l_j$ are the weight of an input parameter k and the bias for node $k$ in layer $l$ respectively. The layered structure of the neural network ensures that the inputs for the nodes in layer $l$ depend only on the outputs of the nodes from layer $l-1$, hence:
\begin{equation}
\label{eq::layer_dep}
a^l_j=\sigma\left(\sum_k w^l_{jk}a^{l-1}_k + b_j^l \right) = \sigma(z^l_j),
\end{equation}
where the linear weighted sum is denoted as:
\begin{equation}
\sigma(z^l_j)=\sum_k w^l_{jk}a^{l-1}_k + b_j^l.
\end{equation}
The cost function E is computed from the output of the neural network, so it directly depends only on the values of $a_j^L$. Let us define the error $\Delta_j^L$ of the j-th node in the output (L-th) layer as a change in the cost function with respect to the weighted output of the last layer:
\begin{equation}
\label{eq::bp1}
\Delta^L_j=\frac{\partial E}{\partial z_j^L}.
\end{equation}
At the same time the loss depends indirectly on all the preceding layers, so keeping in mind eq. \ref{eq::layer_dep} we can define the error of an arbitrary node $j$ in arbitrary layer $l$ as the change in the cost function E with respect to the weighted input $z^l_j$:
\begin{equation}
\Delta^l_j=\frac{\partial E}{\partial z_j^l}=\frac{\partial E}{\partial a^l_j}\sigma'(z^l_j),
\end{equation}
where $\sigma'(z^l_j)$ is the derivative of the non-linear activation function $\sigma$ with respect to its input at $z^l_j$. But on the other hand we can also interpret the error function  $\Delta^L_j$ in terms of bias partial derivatives:
\begin{equation}
\label{eq::bp2}
\Delta^l_j=\frac{\partial E}{\partial z_j^l}=\frac{\partial E}{\partial b^l_j}\frac{\partial b_j^l}{\partial z_j^l}=\frac{\partial E}{\partial b^l_j}\cdot \textbf{1}.
\end{equation}
So starting from the output layer we can compute the error in any layer $l$, provided we know it for the subsequent layer $l+1$:
\begin{equation}
\label{eq::bp3}
\begin{array}{lcl} 
\Delta^l_j=\frac{\partial E}{\partial z_j^l}=\sum_k \frac{\partial E}{\partial z^{l+1}_j}\frac{\partial z^{l+1}_j}{\partial z_j^l}=\\
=\sum_k \Delta^l_j \frac{\partial z^{l+1}_j}{\partial z_j^l} \left( \sum_k \Delta^l_j w^{l+1}_{kj}\right) \sigma'(z_j^l).
\end{array}
\end{equation}
And finally we can get the gradient of the cost function E with respect to a weight of an arbitrary neuron:
\begin{equation}
\label{eq::bp4}
\frac{\partial E}{\partial w_{jk}^l}=\frac{\partial E}{\partial z^l_j}\frac{\partial z_j^l}{\partial w_{jk}^l}=\Delta^l_j a_k^{l-1}.
\end{equation}
Using these four equations (\ref{eq::bp1}, \ref{eq::bp2}, \ref{eq::bp3}, \ref{eq::bp4}) it is possible to "backpropagate" the error back from the output layer and once we can compute the gradient - we know how we should tune the weights and biases in order to minimize the loss function.
\subsection{Batch normalization}
Batch normalization is a regularization scheme that helps to improve the speed and stability of the \gls{dnn} training. The main idea behind the method is to prevent an \textit{internal covariant shift} - a change in the distribution of network activations due to the change in network parameters during training by means of normalization of the parameters transferred from layer $l$ to layer $l+1$  \cite{batch_normalization}. So let us consider a layer $l$ that has $d$ inputs $\textbf{x}=(x^1,x^2,...,x^d)$, then for every $x^k$ we perform the following transformation:
\begin{equation}
\label{eq::bn1}
\hat{x}^k=\frac{x^k-E[x^k]}{\sqrt{Var[x^k]}},
\end{equation}
where $E[x^k]$ and $Var[x^k]$ are the expectation and variance of the parameter $x$, calculated over the training dataset, respectively. At the same time, we have to be sure that we preserve the non-linearity of the activation function output. In order to do this the two additional parameters are introduced:
\begin{equation}
\label{eq::bn2}
y^k=\gamma  \hat{x}^k + \beta^k ,
\end{equation}
where the parameters $\gamma$ and $\beta$ are trained just like the rest of the network parameters.
Practically if the training is performed within the mini-batch scheme with batch size $B={x_1,...,x_m}$ the batch normalization layer is inserted between the \gls{dnn} layers the transformations for the input $x$ are the following:
\begin{equation}
\begin{array}{lcl} 
\frac{1}{m}\sum_{i=1}^m x_i \rightarrow \mu_B\\
\frac{1}{m}\sum_{i=1}^m (x_i - \mu_B)^2 \rightarrow \sigma^2_B\\
\frac {x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}} \rightarrow \hat{x_i}\\
\gamma \hat{x_i} + \beta \rightarrow y_i \equiv BN_{\gamma,\beta}(x_i),
\end{array}
\end{equation}
where $\epsilon$ is a small regularization constant. 
\section{The hadronic recoil regression }
Considering that hadronic recoil is an observable that uses many inputs from \gls{id}, \gls{emc} and \gls{hc} it is reasonable to expect improvement of the result using modern \gls{mva} techniques. 
\subsection{Input features and model}
 Training, testing and validation was performed using a simulated MC sample $W^+\rightarrow\mu\nu$ at 13 TeV, following the selection presented in Section \ref{sec:samples_selection}. Out of 15 768 239 events that have passed the selection 12 734 109 were used for training and 3 034 130 for testing the performance. 
 Below is the list of 38 input features: 
 \begin{itemize}
 \item \textbf{Vector sums} of charged \gls{pfos}, neutral \gls{pfos} (see Section \ref{sec:pfo}) and the vector sum of both. All three vector sums are included into the input features having two Cartesian components each, making 6 input features. 
 \item \textbf{Transverse energy sum} $\sum E_T$ is also defined in three similar ways, adding three input features.
 \item \textbf Cartesian components of the {two leading jets} momenta in the transverse plane. The jets were demanded to have $p_T>20$ GeV. If one or both jets don't make the cut or there is less than two jets in the event - the corresponding features were assigned zero value. 
 \item Cartesian components of the {five leading \gls{npfos} and five leading \gls{cpfos}} momenta in the transverse plane. 
 \item Number of primary vertices in the event.
 \item Pile-up value $\mu$.
 \item Total number of jets in the event.
 \item Total number of \gls{npfos} and \gls{cpfos} in the event.
\end{itemize}
All input features were pre-processed using the StandardScaler module from Scikit Learn package \cite{scikit-learn}.\\
The model contains 3 dense layers with 256 neurons each, alternated with batch normalization layers (see Fig. \ref{fig::nnmodel}). Using batch normalization layers has allowed to reduce the training time by a factor 10. 
\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.5\textwidth,keepaspectratio]{model.png}
	\caption{A model of the \gls{dnn} used in the analysis. }
	\label{fig::nnmodel}
\end{figure}
The model has used Adam optimizer with learning step $0.001$ and batch size of $4000$ data points. Twenty percent of events were used for validation. The two target values were Cartesian components of the truth $p_T^W$ vector. 

The loss function used is the mean square error, which means that for every batch of size $B$ the loss is defined as:
\begin{equation}
 \mathcal{L}=\sum_{i=1}^B(x_i^{pred}-x_i^{target})^2,
\end{equation}
where $x_i^{target}$ and $x_i^{pred}$ are the target (truth) and predicted values for the $i^{th}$ event respectively. For our case of two target values, namely $u_X$ and $u_Y$ the loss takes the following form:
\begin{equation}
\mathcal{L}=\sum_{i=1}^B\left((x_i^{pred}-x_i^{target})^2+(y_i^{pred}-y_i^{target})^2\right).
\end{equation}
Figure \ref{fig::nntraining} shows dependence of loss on training epochs. Eventually the model with weights obtained after 38 epochs of training was used in the analysis. 
\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.6\textwidth,keepaspectratio]{loss.png}
	\caption{Learning curve of the model.}
	\label{fig::nntraining}
\end{figure}
\subsection{Kinematic distributions} 
The results presented here show the regression plots obtained with the trained DNN. The regression was tested for the four W channels in MC and for the two Z channels in both data and MC. Below the plots for $W^-\rightarrow \mu \nu$ channel are presented, the rest of the results are presented in Appendix B.

In Figure \ref{fig:mva_deltas} the difference between the reconstructed hadronic recoil and truth distributions (targets) are shown. For the two target components $u_X$ and $u_Y$ a sharper peak centred at zero is observed in the MVA-reconstructed recoil comparing to the standard algorithm. Similar comparison is shown in Fig. \ref{fig:mva_deltas_polar} for HR vector components in polar coordinates. The polar angle $u_{\phi}$ of the HR vector shows a very small, nearly negligible improvement from the MVA. The $u_T$ vector magnitude demonstrates a shift from zero for the $u_T^{MVA}$, indicating a bias. Figure \ref{fig:mva_upar_pT} shows that the MVA-reconstructed recoil has a softer spectrum comparing to both $p_T^{truth}$ and standard recoil spectra.

The bias is shown in Fig. \ref{fig:mva_bias_uperp} together with the $u_{perp}$ component. Indeed the $u_T^{MVA}$ demonstrates a larger bias as compared to the standard algorithm. At the same time the standard deviation of the $u_{\perp}$ component indicates an improvement in the resolution.

	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{histosDeltaUxminusmunu}%
	\includegraphics[width=.49\textwidth]{histosDeltaUyminusmunu}
	\caption{The difference between the Cartesian components of the HR vector for the standard $u_T$ and $u_T^{MVA}$.}
	\label{fig:mva_deltas}
\end{figure}
	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{histosDeltaUPhiminusmunu}%
	\includegraphics[width=.49\textwidth]{histosDeltaUtminusmunu}
	\caption{The difference between the polar components of the HR vector for the standard $u_T$ and $u_T^{MVA}$.}
	\label{fig:mva_deltas_polar}
\end{figure}
	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{hist_uTminusmunu}%
	\includegraphics[width=.49\textwidth]{outMVA_Uparminusmunu.pdf}
	\caption{Comparison of $u_T$ and $u_T^{MVA}$ spectra and response.}
	\label{fig:mva_upar_pT}
\end{figure}
	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{outMVA_biasminusmunu}%
	\includegraphics[width=.49\textwidth]{outMVA_uPerpminusmunu}
	\caption{Bias and $u_{\perp}$ for the standard $u_T$ and $u_T^{MVA}$.}
	\label{fig:mva_bias_uperp}
\end{figure}
The dependence of the bias and $u_{\perp}$ on the momentum is studied in Fig. \ref{fig:biasuperp_vs_pt}. The $u_T^{MVA}$ recoil demonstrates improvement in the resolution and a larger bias in the region of $p_T<80\gev$. However, for a quantitative resolution comparison we need to make sure that $u_{\perp}$ and $u_{\perp}^{MVA}$ are on the same scale. A possible way to achieve this is to normalize them to average recoil $<u_T>$. The resulting normalized curves on Fig. \ref{fig:uperp_relative} show that the MVA provides 5-10\% resolution improvement at $p_T<10\gev$ and a bit more than 10\% in $10<p_T<30\gev$ transverse momentum region. \\
	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{histosMeanBiasminusmunu.pdf}%
	\includegraphics[width=.49\textwidth]{histosSigmaUperpminusmunu.pdf}
	\caption{Bias and $u_{\perp}$ as a function of $p_T^{truth}$.}
	\label{fig:biasuperp_vs_pt}
\end{figure}

	\begin{figure}[tph]
	\centering
	\includegraphics[width=.6\textwidth]{histosUperpRelativeminusmunu.pdf}%
	\caption{Normalized $\frac{u_{\perp}}{<u_T>}$  as a function of $p_T^{truth}$.}
	\label{fig:uperp_relative}
\end{figure}
The same neural network was applied for the HR regression for both Z channels, for data and MC events. In case of data the $p_T^{truth}$ vector was replaced with \ptll. A qualitatively similar picture holds for both Z channels, confirming recoil universality for W and Z events (see Fig. \ref{fig:mva_Zbenchmark}). The complete set of $u_T$ vs $u_T^{MVA}$ comparison plots for all W and Z channels can be found in Appendix B.
	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{histosUperpRelativezeemc}%
	\includegraphics[width=.49\textwidth]{histosUperpRelativezeedata}
	\includegraphics[width=.49\textwidth]{histosUperpRelativezmumumc}%
	\includegraphics[width=.49\textwidth]{histosUperpRelativezmumudata}
	\caption{Relative resolution for Z channels in MC (left) and in data (right).}
	\label{fig:mva_Zbenchmark}
\end{figure}
\clearpage
\section{Rescaling of the MVA hadronic recoil }
As we can see from the pT plot the average scale of the MVA-reconstructed hadronic recoil is smaller than that of the nominal hadronic recoil. This leads to an increased bias. Using the $u_{\parallel}-p_T$ dependence we can rescale the MVA hadronic recoil to match the nominal recoil scale. 

	\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{Scaled/hist_uXplusmunu.pdf}%
	\includegraphics[width=.49\textwidth]{Scaled/hist_uYplusmunu.pdf}
	\includegraphics[width=.49\textwidth]{Scaled/hist_uTplusmunu.pdf}%
	\includegraphics[width=.49\textwidth]{Scaled/outMVA_Uparplusmunu.pdf}
	\caption{The target distributions $u_x$ and $u_y$ (top plots). Bottom plots show th uT spectra (left) and $u_{\parallel}$ distributions (right).}
	\label{fig:mva_scaled_xy}
\end{figure}
\begin{figure}[tph]
		\centering
		\includegraphics[width=.49\textwidth]{Scaled/histosDeltaUxplusmunu.pdf}%
		\includegraphics[width=.49\textwidth]{Scaled/histosDeltaUyplusmunu.pdf}
		\includegraphics[width=.49\textwidth]{Scaled/histosDeltaUtplusmunu.pdf}%
		\includegraphics[width=.49\textwidth]{Scaled/outMVA_biasplusmunu.pdf}
		\caption{Difference plots between the target and truth values.}
		\label{fig:mva_scaled_deltas}
\end{figure}

\begin{figure}[tph]
	\centering
	\includegraphics[width=.49\textwidth]{Scaled/histosMeanUparplusmunu.pdf}%
	\includegraphics[width=.49\textwidth]{Scaled/histosSigmaUperpplusmunu.pdf}
	\includegraphics[width=.49\textwidth]{Scaled/histosMeanBiasplusmunu.pdf}%
	\includegraphics[width=.49\textwidth]{Scaled/histosUperpRelativeplusmunu.pdf}
	\caption{Bias, $u_{\parallel}$, $\sigma(u_{\perp})$ and $\sigma(u_{\perp})/<u_T>$ dependencies on the $p_T^{truth}$.}
	\label{fig:mva_scaled_mean}
\end{figure}
The plots demonstrate that rescaling of the MVA-derived hadronic recoil makes the average bias roughly the same as in the nominal hadronic recoil. At the same time the relative resolution retains the improvement seen in the non-scaled MVA hadronic recoil.
\clearpage
\section{Template fits for W boson mass}
A way to estimate the improvement is to test if the new hadronic recoil definition results in the improved sensitivity for the W boson mass. In order to do this a set of templates is produced where the mass of the W boson is shifted from its nominal value. The kinematic distributions are reweighted to reflect the shift of the W mass. 
Every template is compared to the nominal distribution using the $\chi^2$ test. The more sensitive is the observable to the $m^W$ change - the larger is the $\chi^2$ value of the corresponding template. The procedure is described in detail in the W mass paper internal note~\cite{Wmass7TeVIntNote}. In Fig. \ref{fig:chi2_parabola} an example of the $\chi^2$ parabola fit is demonstrated. The MVA-derived hadronic recoil shows a considerably higher sensitivity to the W boson mass shift. 
\begin{figure}[tph]
	\centering
	\includegraphics[width=0.8\textwidth]{met_mass_plusmunu_cut7.pdf}
	\caption{An example of a $\chi^2$ fit of the $E_T^{miss}$ distribution templates for the three hadronic recoil definitions.}
	\label{fig:chi2_parabola}
\end{figure}

The next step is to produce bootstrap toys in order to estimate the statistical uncertainty of the W mass fits and the correlation between the observables. For every HR definition a set of 1000 toys has been generated. Every toy is fitted using the generated templates, providing a value for the W mass. Putting these values onto a 2D scatter plot allows to estimate the correlation. The standard deviation of the distributions provides the statistical uncertainty.

\begin{figure}[tph]
	\centering
	\includegraphics[width=.33\textwidth]{Scaled/Nominal_HR_plT_vs_mT.pdf}%
	\includegraphics[width=.33\textwidth]{Scaled/MVA_HR_plT_vs_mT.pdf}
	\includegraphics[width=.33\textwidth]{Scaled/MVA_scaled_HR_plT_vs_mT.pdf}
	\includegraphics[width=.33\textwidth]{Scaled/Nominal_HR_met_vs_mT.pdf}%
	\includegraphics[width=.33\textwidth]{Scaled/MVA_HR_met_vs_mT.pdf}
	\includegraphics[width=.33\textwidth]{Scaled/MVA_scaled_HR_met_vs_mT.pdf}
	\includegraphics[width=.33\textwidth]{Scaled/Nominal_HR_met_vs_lpT.pdf}%
	\includegraphics[width=.33\textwidth]{Scaled/MVA_HR_met_vs_lpT.pdf}
	\includegraphics[width=.33\textwidth]{Scaled/MVA_scaled_HR_met_vs_lpT.pdf}
	\caption{Scatter plots that illustrate the correlation between the observables for the three hadronic recoil definitions: Nominal (left column), MVA (central column) and scaled MVA (right column).}
	\label{fig:mva_scatter}
\end{figure}

Here we see that the smaller scale of the MVA hadronic recoil increases the correlation between the observables. On the other hand the transverse mass and missing transverse energy uncertainties are smaller in the MVA case.

\begin{table}
	\centering
	\begin{tabular}{|c| c| c| c|}
		\hline
		Stat. uncertainty, [MeV] & $m_T$ & $E_T^{miss}$  & $p_T^{l}$ \\
		\hline
		Nominal & 4.54 &  8.76 & 5.44\\
		\hline
		MVA & 4.15 & 5.3 & 5.43\\
		\hline
		Scaled MVA & 4.52 & 7.88 & 5.41 \\
		\hline
	\end{tabular}\\
	\caption{Estimate of the Z vertex reweighting effect on the W boson mass.}
	\label{tab::mva_template}
\end{table}
